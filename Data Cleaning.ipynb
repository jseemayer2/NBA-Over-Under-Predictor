{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usable Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted([f for f in os.listdir('csv') if '.csv' in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_odds(file):\n",
    "    '''\n",
    "    cleans csv file and returns dataframe object that contains the over/under lines (target)\n",
    "    '''\n",
    "    df = pd.read_csv(file)\n",
    "    df = df.iloc[:,:13] #some files contained unnamed extra rows\n",
    "    df.dropna(axis=0,inplace=True) #and others columns\n",
    "    \n",
    "    #makes a list of game_id's equivalent to how many games were played that season\n",
    "    mylist = []\n",
    "    for i in range(1, int(df.shape[0]/2 + 1)):\n",
    "        mylist.append(i)\n",
    "        mylist.append(i)\n",
    "    df['game_id'] = mylist #maps game_id's to games (spread across 2 rows)\n",
    "    \n",
    "    df = df.merge(df, on='game_id', suffixes=(None,'_2')) #gets games to be in one row (creates 4/game)\n",
    "    df = df[1::4] #gets correct mapping\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #make dates include year so we can join with another dataset later\n",
    "    df['Date'] = df['Date'].astype('int')\n",
    "    df['Date'] = df['Date'].astype('str')\n",
    "    #df['Date'] = df['Date'].str.strip('.0')\n",
    "    dates = []\n",
    "    for date in df['Date']:\n",
    "        if int(date) > 1000:\n",
    "            date = '20'+file[:2]+date #GET FILENAMES AND USE HERE\n",
    "            dates.append(date)\n",
    "        else:\n",
    "            date = '20'+file[3:5]+'0'+date #GET FILENAMES AND USE HERE\n",
    "            dates.append(date)\n",
    "    df['Date'] = dates\n",
    "    \n",
    "    #take out pick-em's and replace with zero for later transformation\n",
    "    df['Open'].replace(['PK','pk'],'0',inplace=True)\n",
    "    df['Close'].replace(['PK','pk'],'0',inplace=True)\n",
    "    df['Open_2'].replace(['PK','pk'],'0',inplace=True)\n",
    "    df['Close_2'].replace(['PK','pk'],'0',inplace=True)\n",
    "    df.replace('197.5u10','197.5',inplace=True) #one unique occurrence\n",
    "    #map strings as floats for comparison\n",
    "    df['Open'] = df['Open'].astype('float')\n",
    "    df['Open_2'] = df['Open_2'].astype('float')\n",
    "    df['Close'] = df['Close'].astype('float')\n",
    "    df['Close_2'] = df['Close_2'].astype('float')\n",
    "    #get correct over/under line at opening and closing of sportsbook\n",
    "    df['O/U_open'] = np.where(df['Open'] > df['Open_2'],df['Open'],df['Open_2'])\n",
    "    df['O/U_close'] = np.where(df['Close'] > df['Close_2'],df['Close'],df['Close_2'])\n",
    "    \n",
    "    #rename columns to correct home/visitor \n",
    "    df['Visitor'] = df['Team']\n",
    "    df['Home'] = df['Team_2']\n",
    "    \n",
    "    #add season column\n",
    "    df['Season'] = file[:2] + file[3:5]\n",
    "    \n",
    "    #drop unneeded info\n",
    "    df = df[['Date','Home','Visitor','O/U_open','O/U_close','Season']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this clean_odds function to generate our dataset to combine with our scraped game data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cd csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = pd.concat([clean_odds(f) for f in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lines.reset_index(drop=True,inplace=True)\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pickle this initial processing of the csv's and subsequent DataFrame. We can always comeback to this if we need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lines.pickle', 'wb') as to_write:\n",
    "    pickle.dump(lines, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('game_df.pickle','rb') as read_file:\n",
    "    game_df = pickle.load(read_file)\n",
    "    \n",
    "game_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_abbrev = game_df['home'].unique()\n",
    "team_abbrev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(team_abbrev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines['Home'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_dict = {\n",
    "    'SAS': 'SanAntonio',\n",
    "    'GSW': 'GoldenState',\n",
    "    'LAL': 'LALakers',\n",
    "    'TOR': 'Toronto',\n",
    "    'IND': 'Indiana',\n",
    "    'ORL': 'Orlando',\n",
    "    'NJN': 'NewJersey',\n",
    "    'CLE': 'Cleveland',\n",
    "    'MEM': 'Memphis',\n",
    "    'NOH': 'NewOrleans',\n",
    "    'NOP': 'NewOrleans',\n",
    "    'DEN': 'Denver',\n",
    "    'MIA': 'Miami',\n",
    "    'UTA': 'Utah',\n",
    "    'SEA': 'Seattle',\n",
    "    'CHA': 'Charlotte',\n",
    "    'CHO': 'Charlotte',\n",
    "    'ATL': 'Atlanta',\n",
    "    'BOS': 'Boston',\n",
    "    'MIN': 'Minnesota',\n",
    "    'CHI': 'Chicago',\n",
    "    'PHO': 'Phoenix',\n",
    "    'LAC': 'LAClippers',\n",
    "    'PHI': 'Philadelphia',\n",
    "    'WAS': 'Washington',\n",
    "    'MIL': 'Milwaukee',\n",
    "    'HOU': 'Houston',\n",
    "    'DAL': 'Dallas',\n",
    "    'NYK': 'NewYork',\n",
    "    'DET': 'Detroit',\n",
    "    'SAC': 'Sacramento',\n",
    "    'POR': 'Portland',\n",
    "    'OKC': 'OklahomaCity',\n",
    "    'BRK': 'Brooklyn'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_df['home'].map(team_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_df['home_team'] = game_df['home'].map(team_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "game_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_df['game_date'] = game_df['game_id'].apply(lambda x: x[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_df['id'] = game_df['game_date'] + game_df['home_team']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines['id'] = lines['Date'] + lines['Home']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "game_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now both databases have a column to merge on. The 'id' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(lines,game_df,on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns that share the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Home','Visitor','game_id','home_team','game_date'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the difference between the line and the outcome of a particular game in order to classify our game as an \"Over\" or \"Under\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ou1'] = df['total'] - df['O/U_open']\n",
    "df['ou2'] = df['total'] - df['O/U_close']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to label our target based off information we have in our DataFrame, and then applying that function to our newly created columns that currently classify our games based on a positive or negative value. This simply puts a more general categorical label over that more granular scalar indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_under(ou):\n",
    "    if ou > 0:\n",
    "        return 1\n",
    "    elif ou == 0:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Over/Under_open'] = df['ou1'].apply(over_under)\n",
    "df['Over/Under_close'] = df['ou2'].apply(over_under)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRITICAL ASSUMPTION: Making the decision to drop \"pushes\". A \"push\" is when a betting line is hit exactly, and no money exchanges hands. In the context of this project, a total could be at 212 points, and if the game finishes with exactly 212 points, then every bettor gets their money back, on both sides (over/under 212), and the sportsbook doesn't collect anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Over/Under_open'] != 2]\n",
    "df = df[df['Over/Under_close'] != 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 400 instances of a \"push\", or about 2.7% of our original dataset. This number can be remembered for sampling or simulation purposes later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert date to pd.datetime object. Makes plotting the time series compatible with matplotlib, and building season dictionary later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stat building (feature engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to ultimately end up with a model that takes both teams' recent performances and can make a prediction on the total, so we can make a new column to hold home/visitor every other row, and then rename the _h and _v columns for self, opponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df.reindex(df.index.repeat(2)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have every game repeated, let's create a column to indicate which team these self/opponent stats represent. We can do this by creating an empty column and selectively copy from the 'home' and 'away' columns that repeat across the 2 rows. Then we'll do that same for the opponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['team'] = np.nan\n",
    "#home team will be all even indexes of this dataset\n",
    "d['team'][::2] = d['home'][::2]\n",
    "#away team will be all odd indexes of this dataset\n",
    "d['team'][1::2] = d['away'][1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['opp'] = np.nan\n",
    "d['opp'][::2] = d['away'][::2]\n",
    "d['opp'][1::2] = d['home'][1::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep track of home/away, can't hurt. We'll use the same process as we just used. Note: this can also be used as a sanity check to make sure that our splitting worked correctly. If it did, we should see 'H', 'V' every other row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['home/away'] = np.nan\n",
    "#home team will be all even indexes of this dataset\n",
    "d['home/away'][::2] = 'H'\n",
    "#away team will be all odd indexes of this dataset\n",
    "d['home/away'][1::2] = 'V'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to map the _v and _h stats appropriate to new columns for self or opponent stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ['pace','eFg','tov','orb','ft_fga','ortg','fg','fga','fg_per','threes','threes_att','threes_per',\n",
    "         'ft','fta','ft_per','drb','trb','ast','stl','blk','to','fouls','ts_per','threes_ar','ft_ar',\n",
    "         'drb_per','trb_per','ast_per','stl_per','blk_per','user_per','drtg']\n",
    "for stat in stats:\n",
    "    d['{}'.format(stat)] = \"\"\n",
    "    d['{}'.format(stat)][::2] = d['{}_h'.format(stat)][::2]\n",
    "    d['{}'.format(stat)][1::2] = d['{}_v'.format(stat)][1::2]\n",
    "    d['{}_opp'.format(stat)] = \"\"\n",
    "    d['{}_opp'.format(stat)][::2] = d['{}_v'.format(stat)][::2]\n",
    "    d['{}_opp'.format(stat)][1::2] = d['{}_h'.format(stat)][1::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can drop all of our columns with '_v' & '_h' since they contain extra, and now, redundant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = []\n",
    "for stat in stats:\n",
    "    mylist.append('{}_v'.format(stat))\n",
    "    mylist.append('{}_h'.format(stat))\n",
    "#print(mylist)\n",
    "d = d.drop(columns=mylist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our DataFrame (d) has stats for each team and their opponent for every game. Now let's get some rolling averages. First we'll create an empty column that we'll populate with a rolling count for every time that team has appeared. This will get us the number of games each team has played up INCLUDING that game for each season. We can use that info to incorporate some rolling averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['team_season'] = d['team'] + d['Season']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['game_num'] = d.groupby('team_season').cumcount()+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to compute some rolling average's in a certain season. Remember, we want to get a snapshot of how both team's have perfromed recently (last 5 games), and get a classification from this info. So let's go back to our method of creating empty columns we can assign data to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stat in d.iloc[:,15:79]:\n",
    "    d['{}_rolling'.format(stat)] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast our stat columns as floats to perform some operations on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in d.iloc[:,15:79].columns:\n",
    "    d[col] = d[col].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may not be the most pythonic way to populate these new columns, but it works, and doesn't take long. At a high level, we are going team by team, then season by season, for that team, and applying 5 game rolling averages to their stats and opponent stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for team in d['team'].unique():\n",
    "    mask = d['team'] == team\n",
    "    d4 = d[mask]\n",
    "    for season in d4['Season'].unique():\n",
    "        mask = d4['Season'] == '{}'.format(season)\n",
    "        d5 = d4[mask]\n",
    "        for stat in list(d5.iloc[:,15:79].columns):\n",
    "            d5['{}_rolling'.format(stat)] = d5.rolling(window=5)['{}'.format(stat)].mean().shift(1)\n",
    "        d.update(d5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT NOTE: this DataFrame contains rows with NaN's. Every game that is in the first 5 of the season has no rolling averages, this can be problematic when we take this DataFrame and try to feed it into a sk-learn ML model. We can address this now, by simply dropping these games(~5% of our dataset), or apply the previous season's median rolling average. This is where some decisions need to be made, and I would advise just dropping the games for 2 reasons. One, taking out 5 games won't change the application our model will have as there are 67-75 games left in the year we can apply this model to once in production. Two, a median of the rolling averages, or any other kind of congregation statistic being applied over different seasons is mostly likely not a sound choice in light of team personnel turnover, from retirings, trades, the draft, and free agent moves; not to mention new coaching staffs. This could be applicable to a few teams (ones that experience little of this roster turnover), but not enough to apply that thinking across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't cut off these rows in this notebook, but can easily do so in our modeling notebook by using the following mask and declaring this filtered d as our new d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[d['game_num']>5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final thing we have to do before we can begin classifying our games is make each row one game. Our features will include rolling averages for each of our team's and opponent's stats. From here we can get a baseline sense if our logic, of recent past performances, is good basis for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is fully processed, we are ready to process this in a classification algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('d_rolling.pickle', 'wb') as to_write:\n",
    "    pickle.dump(d, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:,:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 101)\n",
    "pd.set_option(\"display.max_rows\", 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = []\n",
    "for i in range(1, int(df.shape[0]/2 + 1)):\n",
    "    mylist.append(i)\n",
    "    mylist.append(i)\n",
    "print(mylist[0],mylist[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(range(1,int(df.shape[0]/2 + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['game_id'] = mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.merge(df, on='game_id', suffixes=(None,'_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[1::4]\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check that every game only appears once\n",
    "df['game_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = df['Date'].astype('int')\n",
    "df['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Date'] = df['Date'].astype('str')\n",
    "df['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = df['Date'].str.strip('.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = []\n",
    "for date in df['Date']:\n",
    "    if int(date) > 1000:\n",
    "        date = '20'+files[2][:2]+date\n",
    "        dates.append(date)\n",
    "    else:\n",
    "        date = '20'+files[2][3:5]+'0'+date\n",
    "        dates.append(date)\n",
    "df['Date'] = dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['Visitor'] = df['Team']\n",
    "df['Home'] = df['Team_2']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Open'].replace('pk','0',inplace=True)\n",
    "df['Close'].replace('pk','0',inplace=True)\n",
    "df['Open_2'].replace('pk','0',inplace=True)\n",
    "df['Close_2'].replace('pk','0',inplace=True)\n",
    "df.replace('197.5u10','197.5',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df['Open'].values == 'pk'\n",
    "df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Open'] = df['Open'].astype('float')\n",
    "df['Open_2'] = df['Open_2'].astype('float')\n",
    "df['Close'] = df['Close'].astype('float')\n",
    "df['Close_2'] = df['Close_2'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['O/U_open'] = np.where(df['Open'] > df['Open_2'],df['Open'],df['Open_2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['O/U_close'] = np.where(df['Close'] > df['Close_2'],df['Close'],df['Close_2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check that opening and closing lines operations worked successfully.\n",
    "So long as numbers are around 200, we know we are ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['O/U_open'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['O/U_close'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Date','Home','Visitor','O/U_open','O/U_close']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing our lines df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, and most importantly, we must create our target labels. This will be done by comparing the total the the opening and closing lines, and mapping that result to one of three categories: Over, Under, or Push. This will represent the winning result of that game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ou1'] = df['total'] - df['O/U_open']\n",
    "df['ou2'] = df['total'] - df['O/U_close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_under(ou):\n",
    "    if ou > 0:\n",
    "        return 1\n",
    "    elif ou == 0:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Over/Under_open'] = df['ou1'].apply(over_under)\n",
    "df['Over/Under_close'] = df['ou2'].apply(over_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the decision to drop pushes as no money exchanges hands in this scenerio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Over/Under_open'] != 2]\n",
    "df = df[df['Over/Under_close'] != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 400 instances of a push, or about 2.7% of our original dataset. This number can be remembered for sampling or simulation purposes later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stat Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the same set of stats for both teams in any one game, so we can build offense/defense for both teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''map stats accordingly:\n",
    "1. Get 2 \"sets\" of stats per game:\n",
    "\n",
    "visitor: offense - _v stat avgs heading into the game; defense - _h stat avgs heading into the game\n",
    "home: offese - _h stat avgs heading into the game; defense - _v stat avgs heading into the game\n",
    "\n",
    "2. Map visitor/home stats to respective teams\n",
    "\n",
    "3. Build dictionary of team's seasons to be further processed.\n",
    "\n",
    "{1415: {GSW: {..game_35:{offense/defense stats},game_36:{..}\n",
    "\n",
    "4. Process dictionary to have more stats/potential model features:\n",
    "\n",
    "{GSW: {..game_35:{offense/defense stats averaged through 34 games},game_36:{..}\n",
    "\n",
    "5. ?\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert date to pd.datetime object. May help with building season dictionary later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df.reindex(df.index.repeat(2)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have every game twice, we can make a new column to hold home/visitor every other row, and then rename the _h and _v columns for self, opponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the column we want to place our values, then selectively copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['team'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#home team will be all even indexes of this dataset\n",
    "d['team'][::2] = d['home'][::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#away team will be all odd indexes of this dataset\n",
    "d['team'][1::2] = d['away'][1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['opp'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['opp'][::2] = d['away'][::2]\n",
    "d['opp'][1::2] = d['home'][1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[['team','opp']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep track of home/away, can't hurt. We'll use the same process as we just used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['home/away'] = np.nan\n",
    "#home team will be all even indexes of this dataset\n",
    "d['home/away'][::2] = 'H'\n",
    "#away team will be all odd indexes of this dataset\n",
    "d['home/away'][1::2] = 'V'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to map the _v and _h stats appropriate to new columns for self or opponent stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ['pace','eFg','tov','orb','ft_fga','ortg','fg','fga','fg_per','threes','threes_att','threes_per',\n",
    "         'ft','fta','ft_per','drb','trb','ast','stl','blk','to','fouls','ts_per','threes_ar','ft_ar',\n",
    "         'drb_per','trb_per','ast_per','stl_per','blk_per','user_per','drtg']\n",
    "for stat in stats:\n",
    "    d['{}'.format(stat)] = \"\"\n",
    "    d['{}'.format(stat)][::2] = d['{}_h'.format(stat)][::2]\n",
    "    d['{}'.format(stat)][1::2] = d['{}_v'.format(stat)][1::2]\n",
    "    d['{}_opp'.format(stat)] = \"\"\n",
    "    d['{}_opp'.format(stat)][::2] = d['{}_v'.format(stat)][::2]\n",
    "    d['{}_opp'.format(stat)][1::2] = d['{}_h'.format(stat)][1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mylist = []\n",
    "for stat in stats:\n",
    "    mylist.append('{}_v'.format(stat))\n",
    "    mylist.append('{}_h'.format(stat))\n",
    "#print(mylist)\n",
    "d = d.drop(columns=mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our DataFrame (d) has stats for each team and their opponent for every game. Now let's get some rolling averages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll create an empty column that we'll populate with a rolling count for every time that team has appeared. This will get us the number of games each team has played up INCLUDING that game for each season. We can use that info to incorporate some rolling averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['team_season'] = d['team'] + d['Season']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['game_num'] = d.groupby('team_season').cumcount()+1\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast our stat columns as floats to perform some operations on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in d.iloc[:,15:79].columns:\n",
    "    d[col] = d[col].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['team_season_game_num'] = d['team_season'] + d['game_num'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['team_season_game_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stat in d.iloc[:,15:79]:\n",
    "    d['{}_rolling'.format(stat)] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for team in d['team'].unique():\n",
    "    mask = d['team'] == team\n",
    "    d4 = d[mask]\n",
    "    for season in d4['Season'].unique():\n",
    "        mask = d4['Season'] == '{}'.format(season)\n",
    "        d5 = d4[mask]\n",
    "        for stat in list(d5.iloc[:,15:79].columns):\n",
    "            d5['{}_rolling'.format(stat)] = d5.rolling(window=5)['{}'.format(stat)].mean().shift(1)\n",
    "        d.update(d5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[d['game_num']>5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each team, get a single season, calculate rolling averages, then insert into our DataFrame (d). Necessary since every season has potentially different number of games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for season in d4['Season'].unique():\n",
    "    mask = d4['Season'] == '{}'.format(season)\n",
    "    d5 = d4[mask]\n",
    "    d5\n",
    "    for stat in list(d5.iloc[:,15:79].columns):\n",
    "        d5['{}_rolling'.format(stat)] = d5.rolling(window=5)['{}'.format(stat)].mean().shift(1)\n",
    "    #print(len(d4[d4['Season'] == '{}'.format(season)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d5.columns[82:146]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d5.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.update(d5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.iloc[28947:28970,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_list = list(d3.iloc[:,5:].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stat in stat_list:\n",
    "    d3['{}_rolling'.format(stat)] = d3.rolling(window=5)['{}'.format(stat)].mean().shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Work Graveyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing to merge datasets graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tried self-merging on game_id, but this is only applicable if in different columns\n",
    "#df1 = df1.merge(\n",
    "#            right=df1[opp_pull_cols],\n",
    "#            left_on=[\"game_id\", \"team\"],\n",
    "#            right_on=[\"game_id\", \"opp\"],\n",
    "#            suffixes=[None, \"_opp\"],\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tried to use drop_duplicates method but can only keep first or last\n",
    "#df.drop_duplicates(subset=['game_id'], keep='second')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rolling average graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#found online, talks about rolling average based on multiple columns, never tried\n",
    "#df.loc[:, 'value_sma_10'] = df.groupby(by='object')[['object', 'period']].rolling(window=10, min_periods=1, on='period').mean().reset_index(level='object')['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#found online, got it to work, but not quite applicable to this situation\n",
    "#span = 5\n",
    "#sma = d2.rolling(window=span, min_periods=span).mean()[:span]\n",
    "#rest = d2[span:]\n",
    "#pd.concat([sma, rest]).ewm(span=span, adjust=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#didn't work - ValueError: cannot reindex from a duplicate axis\n",
    "#d2['eFg_rolling'] = d2.groupby(['team_season','game_num'])['eFg'].rolling(10).mean().droplevel(level=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = d.copy()\n",
    "#df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = d.groupby(['team_season','game_num']).rolling(5)['eFg'].mean().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df1['pace_rolling'] = d.groupby(['team_season','game_num'])[5:,'pace'].transform(lambda x: x.rolling(10, 10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
